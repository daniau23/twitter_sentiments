{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import nltk\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import gutenberg as cg\n",
    "# sentence tokeniser\n",
    "from nltk.tokenize import word_tokenize as wt \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import textblob\n",
    "import gensim\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no thank you the new vaccine combines the expe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scientificresearch safety and efficacy of dup...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dcp fireextinguisher for all classes kg kg kg ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt discourse health news roundup vietnam repor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health news roundup vietnam reports first case...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned sentiment  \\\n",
       "0  no thank you the new vaccine combines the expe...  negative   \n",
       "1   scientificresearch safety and efficacy of dup...   neutral   \n",
       "2  dcp fireextinguisher for all classes kg kg kg ...  positive   \n",
       "3  rt discourse health news roundup vietnam repor...   neutral   \n",
       "4  health news roundup vietnam reports first case...   neutral   \n",
       "\n",
       "   sentiment_label_values  \n",
       "0                      -1  \n",
       "1                       0  \n",
       "2                       1  \n",
       "3                       0  \n",
       "4                       0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Playing with Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"21297c84ccde408bbaa73626c89db1bb-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-21297c84ccde408bbaa73626c89db1bb-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-21297c84ccde408bbaa73626c89db1bb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-21297c84ccde408bbaa73626c89db1bb-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-21297c84ccde408bbaa73626c89db1bb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-21297c84ccde408bbaa73626c89db1bb-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-21297c84ccde408bbaa73626c89db1bb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print()\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main Preprocessing with Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'who', 'can', 'thence', 'above', 'toward', 'myself', 'or', 'now', 'empty', \"'m\", 'really', 'mine', 'six', 'made', 'themselves', 'before', 'nowhere', 'make', 'neither', 'least', 'in', '’s', 'twelve', 'anything', 'except', 'on', 'ten', 'whither', 'many', 'former', 'often', 'where', 'fifteen', 'which', 'hence', 'see', 'part', 'however', '‘re', 'several', 'noone', 'being', 'and', 'wherein', 'why', 'mostly', 'unless', 'whose', 'already', 'will', 'throughout', 'so', 'please', 'amongst', 'be', 'she', 'eleven', 'whence', 'is', 'for', 'either', 'about', 'even', 'does', 'keep', 'hundred', \"'re\", 'put', 'meanwhile', 'eight', 'him', 'thus', 'if', 'only', '‘ve', 'his', '’m', 'ca', 'seem', 'always', 'my', 'all', 'ourselves', 'some', 'anyone', 'bottom', 'whole', 'since', 'beforehand', 'you', 'of', 'five', 'became', 'next', 'its', 'per', 'up', 'still', 'again', 'though', 'fifty', 'enough', 'under', 'but', 'off', 'too', 'might', 'say', 'besides', 'somewhere', 'by', \"'ve\", 'just', 'nothing', 'three', 'last', 'take', 'elsewhere', 'they', 'down', 'yourselves', 'side', 'sixty', 'then', 'somehow', 'after', 'has', '‘m', 'due', 'beyond', 'also', 'another', 'thereafter', 'quite', 'with', 'doing', 'everywhere', 'very', 'regarding', 'it', 'indeed', 'rather', 'am', 'anyhow', '’re', 'one', 'below', 'get', 'at', 'towards', 'anywhere', 'twenty', 'hers', 'wherever', 'yet', 'those', 'own', \"'ll\", 'a', 'serious', 'hereby', 'perhaps', 'both', 'from', 'yours', 'become', 'such', 'between', 'how', 'therefore', 'further', 'among', 'using', 'whom', 'other', 'herself', 'therein', 'thereupon', 'were', 'hereupon', 'during', 'whoever', '’ll', 'had', 'four', 'than', 'whereby', 'through', 'our', 'ours', 'becomes', 'do', 'would', 'third', 'via', 'should', 'across', \"'d\", 'beside', 'almost', 'first', 'whether', 'out', 'forty', 'never', 'whereafter', 'give', 'was', 'every', 'sometimes', 'afterwards', 'there', 'nobody', 'yourself', '’ve', 'well', 'this', 'otherwise', 'two', 'whereas', 'around', 'their', 'each', 'these', 'nevertheless', 'together', 'your', 'did', 'whatever', 'front', 'into', '‘s', 'name', 'top', 'them', 'done', 'others', 'thereby', 'i', 'less', 'that', 'full', 'her', 'latter', 'although', 'formerly', 'moreover', 'move', 'what', 'call', 'everything', 'to', 'upon', 'itself', 'becoming', 'thru', 'have', 'latterly', 'back', 'are', 'seemed', 'the', '‘ll', 'behind', 'sometime', 'could', 'onto', 'until', 'because', 'same', 'within', 'he', 'me', 'more', 'whenever', 'while', 'various', 'most', 'must', 'ever', 'may', 'amount', 'we', 'nor', 'everyone', 'been', 'us', 'whereupon', 're', 'alone', 'along', 'nine', 'else', 'any', 'few', 'once', 'herein', '‘d', 'someone', 'hereafter', '’d', 'seems', 'when', 'show', 'seeming', 'over', 'something', \"'s\", 'here', 'namely', 'as', 'much', 'used', 'anyway', 'go', 'an', 'himself'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS,'\\n') # <- set of Spacy's default stop words\n",
    "\n",
    "# STOP_WORDS.add(\"your_additional_stop_word_here\")\n",
    "# We remove 'not' because it is needed for the analysis of reviews\n",
    "all_stopwords = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My stops\n",
      " {'who', 'can', 'thence', 'above', 'toward', 'myself', 'or', 'now', 'empty', \"'m\", 'really', 'mine', 'six', 'made', 'themselves', 'before', 'nowhere', 'make', 'neither', 'least', 'in', '’s', 'twelve', 'anything', 'except', 'on', 'ten', 'whither', 'many', 'former', 'often', 'where', 'fifteen', 'which', 'hence', 'see', 'part', 'however', '‘re', 'several', 'noone', 'being', 'and', 'wherein', 'why', 'mostly', 'unless', 'whose', 'already', 'will', 'throughout', 'so', 'please', 'amongst', 'be', 'she', 'eleven', 'whence', 'is', 'for', 'either', 'about', 'even', 'does', 'keep', 'hundred', \"'re\", 'put', 'meanwhile', 'eight', 'him', 'thus', 'if', 'only', '‘ve', 'his', '’m', 'ca', 'seem', 'always', 'my', 'all', 'ourselves', 'some', 'anyone', 'bottom', 'whole', 'since', 'beforehand', 'you', 'of', 'five', 'became', 'next', 'its', 'per', 'up', 'still', 'again', 'though', 'fifty', 'enough', 'under', 'but', 'off', 'too', 'might', 'say', 'besides', 'somewhere', 'by', \"'ve\", 'just', 'nothing', 'three', 'last', 'take', 'elsewhere', 'they', 'down', 'yourselves', 'side', 'sixty', 'then', 'somehow', 'after', 'has', '‘m', 'due', 'beyond', 'also', 'another', 'thereafter', 'quite', 'with', 'doing', 'everywhere', 'very', 'regarding', 'it', 'indeed', 'rather', 'am', 'anyhow', '’re', 'one', 'below', 'get', 'at', 'towards', 'anywhere', 'twenty', 'hers', 'wherever', 'yet', 'those', 'own', \"'ll\", 'a', 'serious', 'hereby', 'perhaps', 'both', 'from', 'yours', 'become', 'such', 'between', 'how', 'therefore', 'further', 'among', 'using', 'whom', 'other', 'herself', 'therein', 'thereupon', 'were', 'hereupon', 'during', 'whoever', '’ll', 'had', 'four', 'than', 'whereby', 'through', 'our', 'ours', 'becomes', 'do', 'would', 'third', 'via', 'should', 'across', \"'d\", 'beside', 'almost', 'first', 'whether', 'out', 'forty', 'never', 'whereafter', 'give', 'was', 'every', 'sometimes', 'afterwards', 'there', 'nobody', 'yourself', '’ve', 'well', 'this', 'otherwise', 'two', 'whereas', 'around', 'their', 'each', 'these', 'nevertheless', 'together', 'your', 'did', 'whatever', 'front', 'into', '‘s', 'name', 'top', 'them', 'done', 'others', 'thereby', 'i', 'less', 'that', 'full', 'her', 'latter', 'although', 'formerly', 'moreover', 'move', 'what', 'call', 'everything', 'to', 'upon', 'itself', 'becoming', 'thru', 'have', 'latterly', 'back', 'are', 'seemed', 'the', '‘ll', 'behind', 'sometime', 'could', 'onto', 'until', 'because', 'same', 'within', 'he', 'me', 'more', 'whenever', 'while', 'various', 'most', 'must', 'ever', 'may', 'amount', 'we', 'nor', 'everyone', 'been', 'us', 'whereupon', 're', 'alone', 'along', 'nine', 'else', 'any', 'few', 'once', 'herein', '‘d', 'someone', 'hereafter', '’d', 'seems', 'when', 'show', 'seeming', 'over', 'something', \"'s\", 'here', 'namely', 'as', 'much', 'used', 'anyway', 'go', 'an', 'himself'}\n"
     ]
    }
   ],
   "source": [
    "# all_stopwords.add('not') # add one stop word at a time\n",
    "# all_stopwords.remove('not') # remove one stop word at a time\n",
    "\n",
    "# Adding several stopwords\n",
    "all_stopwords |= {'not',\"no\", \"n't\", 'n’t','n‘t','cannot','none','without','against'}\n",
    "# Removing several stop words\n",
    "all_stopwords-= {'not',\"no\", \"n't\", 'n’t','n‘t','cannot','none','without','against'}\n",
    "print('My stops\\n',all_stopwords) # has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(all_stopwords) # My own stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preo = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no thank you the new vaccine combines the expe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scientificresearch safety and efficacy of dup...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dcp fireextinguisher for all classes kg kg kg ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt discourse health news roundup vietnam repor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health news roundup vietnam reports first case...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned sentiment  \\\n",
       "0  no thank you the new vaccine combines the expe...  negative   \n",
       "1   scientificresearch safety and efficacy of dup...   neutral   \n",
       "2  dcp fireextinguisher for all classes kg kg kg ...  positive   \n",
       "3  rt discourse health news roundup vietnam repor...   neutral   \n",
       "4  health news roundup vietnam reports first case...   neutral   \n",
       "\n",
       "   sentiment_label_values  \n",
       "0                      -1  \n",
       "1                       0  \n",
       "2                       1  \n",
       "3                       0  \n",
       "4                       0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label_values</th>\n",
       "      <th>cleaned_spacy_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no thank you the new vaccine combines the expe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>no thank new vaccine combine experimental flu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scientificresearch safety and efficacy of dup...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>scientificresearch safety efficacy dupilumab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dcp fireextinguisher for all classes kg kg kg ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>dcp fireextinguisher class kg kg kg kg availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt discourse health news roundup vietnam repor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>rt discourse health news roundup vietnam repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health news roundup vietnam reports first case...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>health news roundup vietnam report case monkey...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned sentiment  \\\n",
       "0  no thank you the new vaccine combines the expe...  negative   \n",
       "1   scientificresearch safety and efficacy of dup...   neutral   \n",
       "2  dcp fireextinguisher for all classes kg kg kg ...  positive   \n",
       "3  rt discourse health news roundup vietnam repor...   neutral   \n",
       "4  health news roundup vietnam reports first case...   neutral   \n",
       "\n",
       "   sentiment_label_values                                  cleaned_spacy_two  \n",
       "0                      -1  no thank new vaccine combine experimental flu ...  \n",
       "1                       0    scientificresearch safety efficacy dupilumab...  \n",
       "2                       1  dcp fireextinguisher class kg kg kg kg availab...  \n",
       "3                       0  rt discourse health news roundup vietnam repor...  \n",
       "4                       0  health news roundup vietnam report case monkey...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/55817040/removing-stop-words-using-spacy\n",
    "\n",
    "df_preo['cleaned_spacy_two'] = df_preo['cleaned'].apply(lambda text: \n",
    "                         \" \".join(token.lemma_ for token in nlp(text) \n",
    "                            if token.text not in stop))\n",
    "df_preo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = df_preo[['sentiment_label_values','cleaned_spacy_two']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_spacy_two</th>\n",
       "      <th>sentiment_label_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no thank new vaccine combine experimental flu ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scientificresearch safety efficacy dupilumab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dcp fireextinguisher class kg kg kg kg availab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt discourse health news roundup vietnam repor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health news roundup vietnam report case monkey...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>scientificresearch health behaviour change u...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>americans worried mentalhealth covid new sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>need covid resource city phoenix cover connect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>fauci admit lockdown negative consequence arti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>scientificresearch herpesviridae lung reacti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     cleaned_spacy_two  sentiment_label_values\n",
       "0    no thank new vaccine combine experimental flu ...                      -1\n",
       "1      scientificresearch safety efficacy dupilumab...                       0\n",
       "2    dcp fireextinguisher class kg kg kg kg availab...                       1\n",
       "3    rt discourse health news roundup vietnam repor...                       0\n",
       "4    health news roundup vietnam report case monkey...                       0\n",
       "..                                                 ...                     ...\n",
       "995    scientificresearch health behaviour change u...                      -1\n",
       "996    americans worried mentalhealth covid new sur...                       0\n",
       "997  need covid resource city phoenix cover connect...                       0\n",
       "998  fauci admit lockdown negative consequence arti...                       0\n",
       "999    scientificresearch herpesviridae lung reacti...                       0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_order = [\n",
    "    'cleaned_spacy_two',\n",
    "    'sentiment_label_values'\n",
    "]\n",
    "df_trans = df_trans[new_order]\n",
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_spacy_two</th>\n",
       "      <th>sentiment_label_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warning issue covid vaccine mrna find breast m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common gene variant link covid mortality covid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>update covid coronavirus booster authorize you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>important covid datum point break prevail na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alot prayer quick speedy recovery cheetay alla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>scientificresearch health behaviour change u...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>americans worried mentalhealth covid new sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>need covid resource city phoenix cover connect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>fauci admit lockdown negative consequence arti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>scientificresearch herpesviridae lung reacti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cleaned_spacy_two  sentiment_label_values\n",
       "0   warning issue covid vaccine mrna find breast m...                       0\n",
       "1   common gene variant link covid mortality covid...                       0\n",
       "2   update covid coronavirus booster authorize you...                       0\n",
       "3     important covid datum point break prevail na...                       0\n",
       "4   alot prayer quick speedy recovery cheetay alla...                       1\n",
       "..                                                ...                     ...\n",
       "95    scientificresearch health behaviour change u...                      -1\n",
       "96    americans worried mentalhealth covid new sur...                       0\n",
       "97  need covid resource city phoenix cover connect...                       0\n",
       "98  fauci admit lockdown negative consequence arti...                       0\n",
       "99    scientificresearch herpesviridae lung reacti...                       0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an unseen data for usage to be predicted on \n",
    "df_predict_set = df_trans.iloc[900:,:2]\n",
    "# df_predict_set.to_frame().reset_index(drop=True)\n",
    "df_predict_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_set.to_csv('predict_set.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_spacy_two</th>\n",
       "      <th>sentiment_label_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>covid vaccination camp precautionary dose az...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>ignore health people die covid alp lnp varia...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>real plague endure bureaucrat chinese covid he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>lead cardiologist warn kid injure vaccine emer...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>late health safety daily thank covid covid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     cleaned_spacy_two  sentiment_label_values\n",
       "895    covid vaccination camp precautionary dose az...                       0\n",
       "896    ignore health people die covid alp lnp varia...                      -1\n",
       "897  real plague endure bureaucrat chinese covid he...                       0\n",
       "898  lead cardiologist warn kid injure vaccine emer...                      -1\n",
       "899         late health safety daily thank covid covid                       1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Train and testing\n",
    "df_trans_new = df_trans.loc[:899,:]\n",
    "df_trans_new.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using CountVectorizer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = vectorizer.fit_transform(df_trans_new.iloc[:,0]).toarray()\n",
    "y = df_trans_new.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving BoW for prediction\n",
    "import pickle\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning the top ten features\n",
    "# vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test= train_test_split(X,y,test_size=.2,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.metrics\n",
    "# sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score,cross_val_predict\n",
    "\n",
    "scv = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also try out Textblob classifier Later on \n",
    "cf = GaussianNB()\n",
    "cf_2 = DecisionTreeClassifier()\n",
    "cf_3 = RandomForestClassifier()\n",
    "# # cf.fit(X_train.toarray(),y_train) # if using TfidfTransformer\n",
    "# cf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.68055556 0.56944444 0.66666667 0.66666667 0.70833333 0.75\n",
      " 0.75       0.63888889 0.73611111 0.76388889]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validatoin\n",
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "cross_val_score(cf, X_train, y_train, cv=scv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.81944444 0.73611111 0.66666667 0.75       0.76388889 0.70833333\n",
      " 0.80555556 0.77777778 0.76388889 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validatoin\n",
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "cross_val_score(cf_2, X_train, y_train, cv=scv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.75       0.70833333 0.70833333 0.70833333 0.69444444 0.77777778\n",
      " 0.76388889 0.73611111 0.75       0.72222222]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validatoin\n",
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "cross_val_score(cf_3, X_train, y_train, cv=scv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cf.fit(X_train.toarray(),y_train) # if using TfidfTransformer\n",
    "cf_3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cf_3, open('cf_3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = cf.predict(X_test.toarray()) # if using TfidfTransformer\n",
    "y_pred = cf_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0,\n",
       "        0,  1,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  1,  0,  0,\n",
       "        0, -1, -1,  0,  0,  0,  1, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  1,  0,  0,  0,\n",
       "        0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
       "       -1,  0,  0, -1,  0, -1, -1,  0,  0, -1,  1,  0,  0,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0, -1, -1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  0,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/22762/understanding-predict-proba-from-multioutputclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5635479244235625"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375945720367486"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6624090898505646"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using TF-IDF** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X  = vectorizer.fit_transform(df_trans_new.iloc[:,0]).toarray()\n",
    "y = df_trans_new.iloc[:,-1]\n",
    "\n",
    "# Saving model for TFIDF\n",
    "pickle.dump(vectorizer, open('vectorizer_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 3630)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # (900, 3630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_3.fit(X_train,y_train)\n",
    "pickle.dump(cf_3, open('cf_3_tfidf.pkl', 'wb'))\n",
    "y_pred = cf_3.predict(X_test)\n",
    "f1_score(y_test,y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7223634784610393"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6405939040085381"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Something important** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't understand what micro and macro average is, just remember the following 'A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using Textblob** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d3784fccdc90acbf957f8297e7e306d4c8b14c1a207bd5307d0795df9a8d77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
